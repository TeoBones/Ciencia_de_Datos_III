# -*- coding: utf-8 -*-
"""MiModulo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N9MFv-zHAJZeP22XNeMT4MJYIAIp-_x4
"""

from google.colab import drive
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.stats.diagnostic import het_breuschpagan, het_white
from scipy.stats import norm
from scipy.stats import shapiro

#Para utilizar RegresionLineal, debemos primero construir una matriz que tiene como columnas nuestras variables predictoras
x1 = 0
x2 = 0

x = np.stack((x1,x2), axis=1)

from google.colab import drive
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.stats.diagnostic import het_breuschpagan, het_white
from scipy.stats import norm
from scipy.stats import shapiro

class Regresion:
    def __init__(self, x, y):
        # x = variables predictora/s
        # y = variable respuesta
        self.x = x
        self.y = y
        self.X = sm.addconstant(x)
        self.__ajuste = False

    def mostrar_estadisticas(self):
        #completar

    def evalua_histograma(self, h, x):
        # Completar

    def predecir(self, new_x): #new_x es una matriz que tiene como columnas los valores particulares de la variable predictora a predecir
        if self.__ajuste == False:
            print("Error. Aplicar el método \".modelar()\" antes de utilizar .predecir()")
        else:
            new_X = sm.addconstant(new_x)
            return (self.modelo).predict(new_X)

    def homocedasticidad(self):
        if self.__ajuste == False:
            print("Error. Aplicar el método \".modelar()\" antes de utilizar .homocedasticidad()")
        else:
            bp_test = het_breuschpagan(self.modelo, self.X)
            bp_value = bp_test[1]
            print("Valor p Homocedasticidad:", bp_value)

    def normalidad(self):
        data = self.resid
        media = np.mean(data)
        desviacion_estandar = np.std(data)

        data_s = (data- media) / desviacion_estandar
        cuantiles_muestrales=np.sort(data_s)
        n=len(data)
        pp=np.arange(1, (n+1))/(n+1)
        cuantiles_teoricos = norm.ppf(pp)

        plt.scatter(cuantiles_teoricos, cuantiles_muestrales, color='blue', marker='o')
        plt.xlabel('Cuantiles teóricos')
        plt.ylabel('Cuantiles muestrales')
        plt.plot(cuantiles_teoricos,cuantiles_teoricos , linestyle='-', color='red')
        plt.show()

        stat, p_valor1 = shapiro("datos")
        print("Valor p normalidad:", p_valor1)

    def intervalos(self, x_new, alfa=0.05):
        X_new = np.insert(x_new, 0, 1)
        prediccion = self.resultado.get_prediction(X_new)

        int_confianza = prediccion.conf_int(alpha=alfa)
        int_prediccion = prediccion.conf_int(obs = True, alpha = alfa)

        print(f"El intervalo de confianza para mu_(Y| {x_new}) es: {int_confianza[0]}")
        print(f"El intervalo de predicción para X = x_new es: {int_prediccion[0]}")
        return int_confianza[0], int_prediccion[0]
#--------------------------------------------------------------------------------------------------------------------#

class RegresionLineal(Regresion):
    def __init__(self, x, y):
        super().__init__(x, y)

    def modelar(self):
        model = sm.OLS(self.y, self.X)
        result = model.fit()
        print(result.summary())

        self.modelo = result
        self.params = result.params()
        self.rsquared_adj = result.rsquared_adj
        self.resid = result.resid
        self.se = result.bse
        self.t_obs = result.#completar
        self.__ajuste = True

    def graficar_recta_ajustada(self): #genera una recta por cada variable predictora
        if self.__ajuste == False:
          print("Error. Aplicar el método \".modelar()\" antes de utilizar .graficar_recta_ajustada()")
        else:
            try:
                (self.x).shape[1]
            except IndexError: #Caso de Regresion Lineal Simple
                plt.scatter(self.x, self.y)
                pred_y = (self.modelo).predict(self.x)
                plt.plot(self.x, pred_y)

            else: #Caso de Regresion Lineal Multiple
                for i in range(self.x.shape[1]):
                    X_aux = sm.addconstant(self.x[:,i]) #Creo un modelo de Regresion Lineal Simple por cada predictora
                    modelo_aux = sm.OLS(self.y, X_aux)

                    plt.scatter(self.x[:,i], self.y)
                    pred_y = modelo_aux.predict(self.x[:,i])
                    plt.plot(self.x[:,i], pred_y)

#--------------------------------------------------------------------------------------------------------------------#

class RegresionLogistica(Regresion):
    def __init__(self, x, y):
        super().__init__(x, y)

    def modelar(self):
        model = sm.Logit(self.y, self.X)
        result = model.fit()
        print(result.summary())

        self.modelo = result
        self.params = result.params()
        self.rsquared_adj = result.rsquared_adj
        self.resid = result.resid
        self.se = result.bse
        self.t_obs = result.#completar
        self.__ajuste = True

    def entrenar(self, k=0.8):
              """
        Entrena el modelo de regresión logística usando la librería statsmodels.

        Args:
            k (float): Porcentaje de datos utilizados para entrenar al modelo, por default k=0.8.

        Returns:
            test_y (pd.DataFrame or np.darray, dtype=int): Respuestas en la muestra de testeo
            pred_y_prob (pd.DataFrame or np.darray, dtype=float): Probabilidad de las predichas en la muestra de testeo

              """
        if isinstance(self.y, pd.DataFrame):
            test_ind = random.sample(range(len(self.y)), int(len(self.y)*k))  # Generamos un array de los "indices" con los que armo el conjunto entrenador y test

            train_X = (self.X).iloc[test_n]  # Conjuntos de entrenamiento
            train_y = (self.y).iloc[test_n]
            test_X = (self.X).drop(test_n)  # Conjuntos de test
            test_y = (self.y).drop(test_n)

        elif isinstance(self.y, np.ndarray):
            indices = np.arange(len(self.y))
            test_ind = np.random.choice(indices, size=int(len(self.y) * (1 - k)), replace=False)

            train_X = np.delete(self.X, test_ind, axis=0)  # Conjuntos de entrenamiento
            train_y = np.delete(self.y, test_ind, axis=0)
            test_X = self.X[test_ind]  # Conjuntos de test
            test_y = self.y[test_ind]

        else:
            print("Error. el conjunto de datos debe ser numpy.ndarray o pandas.DataFrame")
            return None, None

        modelo_train = sm.Logit(train_y, train_X)  # Ajustamos nuestro modelo entrenador
        pred_y_prob = modelo_train.predict(test_X)  # Predicción de las respuestas "test" sobre el modelo ajustando sobre "train"

        return test_y, pred_y_prob

    def matriz_confusion(self, p=-1, k=0.8):
      #k: porcentaje de datos que utilizamos para entrenar al modelo, por default es un 80%
      #p: punto de corte, si p == -1 utiliza el índice de youden

        test_y, pred_y_prob = self.entrenar(k)

        if p == -1: #Encontraremos p a partir del Índice de Youden
            p_max = 0
            aux_max = 0 #en este auxiliar almacenaremos la cuenta: [sensibilidad(p_max) + especificidad(p_max) - 1]
            matriz_confusion_max = 0
            grilla = np.linspace(0,1,100) #grilla con los puntos de cortes a evalauar

            for p in grilla:
                pred_y = 1 * (np.array(y_pred_prob) >= p) #convertimos nuestra prediccion dada por probabilidad en un booleano siguiendo el punto de corte
                matriz_confusion = np.array([[sum((y_pred == 1) & (y_test == 1)), sum((y_pred == 1) & (y_test == 0))], #generamos la matriz de confusion para cada p en grilla
                                             [sum((y_pred == 0) & (y_test == 1)), sum((y_pred == 0) & (y_test == 0))]])

                sensibilidad_p = matriz_confusion[0,0]/(matriz_confusion[0,0] + matriz_confusion[1,0]) #calculamos sensibilidad
                especificidad_p = matriz_confusion[1,1]/(matriz_confusion[1,1] + matriz_confusion[0,1]) #calculamos especificidad

                if (sensibilidad_p + especificidad_p - 1) > aux_max: #si se cumple la condición:
                    p_max = p #actualizamos p_max
                    aux_max = (sensibilidad_p + especificidad_p - 1) #actualizamos aux_max
                    matriz_confusion_max = matriz_confusion

        else:
            pred_y = 1 * (np.array(y_pred_prob) >= p) #convertimos nuestra prediccion dada por probabilidad en un booleano siguiendo el punto de corte
            matriz_confusion = np.array([[sum((y_pred == 1) & (y_test == 1)), sum((y_pred == 1) & (y_test == 0))], #generamos la matriz de confusion para p
                                         [sum((y_pred == 0) & (y_test == 1)), sum((y_pred == 0) & (y_test == 0))]])

        return matriz_confusion